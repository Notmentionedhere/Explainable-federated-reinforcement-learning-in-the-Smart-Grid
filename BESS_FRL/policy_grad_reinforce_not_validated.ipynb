{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8c72a1739630>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import scipy.integrate as integrate\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Sweden Load Data 2005-2017.csv')\n",
    "df.rename({'cet_cest_timestamp':'time','SE_load_actual_tso':'load'},axis='columns',inplace=True)\n",
    "df['time'] = pd.to_datetime(df['time'],errors='ignore', utc=True)\n",
    "df['weekday'] = df['time'].dt.weekday\n",
    "realtime_pv = [0, 0, 0, 0, 0, 0, 0.1, 0.2, 0.4, 0.6, 0.7, 0.9, 1, 1, 0.9, 0.8, 0.6, 0.4, 0.1, 0, 0, 0, 0, 0]\n",
    "realtime_pv = [i * 7200 for i in realtime_pv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env:\n",
    "    def __init__(self,array_loads,array_weekdays,charge=4,action=None,num=7):\n",
    "        self.array_loads = array_loads\n",
    "        self.array_weekdays = array_weekdays\n",
    "        self.charge = charge\n",
    "        self.action = action\n",
    "        self.load_idx = 0\n",
    "        self.day_idx = 0\n",
    "        self.collection_actual_load = []\n",
    "        self.collection_rewards = []\n",
    "        self.day_actual_load = []\n",
    "        self.cost_list=[]\n",
    "        self.reference_list=[]\n",
    "        self.actual_load=None\n",
    "        self.reward = 0\n",
    "        self.episode=0\n",
    "        self.num=num\n",
    "        self.queue = [0 for i in range(self.num)]\n",
    "        \n",
    "    def increase_load_counter(self):\n",
    "        self.load_idx+=1\n",
    "    def clear(self):\n",
    "        self.load_idx = 0\n",
    "        self.day_idx = 0\n",
    "        self.collection_actual_load = []\n",
    "        self.collection_rewards = []\n",
    "        self.day_actual_load = []\n",
    "        self.cost_list=[]\n",
    "        self.reference_list=[]\n",
    "        self.actual_load=None\n",
    "        self.reward = 0\n",
    "        self.episode=0\n",
    "        self.queue = [0 for i in range(self.num)]\n",
    "\n",
    "    def increase_day_counter(self):\n",
    "        self.day_idx+=1\n",
    "        \n",
    "    def generate_input_load(self):\n",
    "        self.input_load = self.array_loads[self.load_idx]\n",
    "\n",
    "    def generate_input_pv(self):\n",
    "        self.input_pv = realtime_pv[self.load_idx % 24]\n",
    "        \n",
    "    def calculate_actual_load(self):\n",
    "        self.charge_amount = 3500\n",
    "\n",
    "        if self.action == None:\n",
    "            self.actual_load = self.input_load - self.input_pv\n",
    "            self.collection_actual_load.append(self.actual_load)\n",
    "            self.increase_load_counter()\n",
    "\n",
    "        elif self.action == 'discharge' and self.charge != 0:\n",
    "\n",
    "            self.actual_load = self.input_load - self.input_pv - self.charge_amount\n",
    "            self.collection_actual_load.append(self.actual_load)\n",
    "            self.increase_load_counter()\n",
    "            self.charge -= 1\n",
    "\n",
    "        elif self.action == 'charge' and self.charge != 4:\n",
    "\n",
    "            self.actual_load = self.input_load - self.input_pv + self.charge_amount\n",
    "            self.collection_actual_load.append(self.actual_load)\n",
    "            self.increase_load_counter()\n",
    "            self.charge += 1\n",
    "\n",
    "        else:\n",
    "            self.actual_load = self.input_load\n",
    "            self.collection_actual_load.append(self.actual_load)\n",
    "            self.increase_load_counter()\n",
    "\n",
    "            \n",
    "    def reward_function_v1(self):\n",
    "        alpha = 0.7\n",
    "        eprice = [0.082, 0.113, 0.17]\n",
    "        t = self.load_idx % 24\n",
    "        # if (t < 6) or (t >= 23):\n",
    "        #     v_0 = 1.025\n",
    "        # if (t >= 6) and (t <= 16):\n",
    "        #     v_0 = 1.01\n",
    "        # if (t > 16) or (t < 23):\n",
    "        #     v_0 = 0.975\n",
    "\n",
    "        if (t < 7) or (t >= 19):\n",
    "            reward = alpha * self.collection_actual_load[-1] * eprice[1] - (1-alpha)* 0.1628* (self.collection_actual_load[-1] - self.collection_actual_load[-2])/3600\n",
    "\n",
    "        if (t >= 11) or (t < 17):\n",
    "            reward = alpha*self.collection_actual_load[-1] * eprice[2] - (1-alpha)* 0.1628*(self.collection_actual_load[-1] - self.collection_actual_load[-2])/3600\n",
    "\n",
    "        if ((t >= 7) and (t < 11)) or ((t >= 17) and (t < 19)):\n",
    "            reward = alpha*self.collection_actual_load[-1] * eprice[3] - (1-alpha)* 0.1628*(self.collection_actual_load[-1] - self.collection_actual_load[-2])/3600\n",
    "\n",
    "\n",
    "        self.cost_list.append(reward)\n",
    "        #self.reference_list.append(cost_input)\n",
    "        return reward\n",
    "    \n",
    "    \n",
    "    def calculate_reward(self):\n",
    "        self.increase_day_counter()\n",
    "        if self.array_weekdays[self.day_idx] == self.array_weekdays[self.day_idx-1]:\n",
    "            self.reward = 0\n",
    "            self.collection_rewards.append(self.reward)\n",
    "        else:\n",
    "            #cost function\n",
    "            self.reward = self.reward_function_v1()\n",
    "            self.collection_rewards.append(self.reward)\n",
    "            self.episode+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    PGN-Policy Gradient Network (Function Approximator):\n",
    "    \n",
    "    - input size: number of observation states in environment\n",
    "    - n_actions: number of actions in environment\n",
    "    \n",
    "    We will use neural network with one hidden layer, which \n",
    "    has one hidden layer with 128 neurons and ReLU activation function, \n",
    "    as was described in report. So, our vector of \\theta parameters \n",
    "    will have 128 elements. \n",
    "    \n",
    "    \"\"\"\n",
    "    # initialization function\n",
    "    def __init__(self, input_size, n_actions):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        #torch.manual_seed(3)\n",
    "        # Creating a simple neural network with 128 parameters \\theta\n",
    "        # in one hidden layer \n",
    "        # Softmax activation will be done in the algorithm later.\n",
    "        #self.net = nn.Sequential(\n",
    "            #nn.Linear(input_size, 512),\n",
    "            #nn.ReLU(),\n",
    "            #nn.Linear(512, 126),\n",
    "            #nn.ReLU(),\n",
    "            #nn.Linear(126, 64),\n",
    "            #nn.ReLU(),\n",
    "            #nn.Linear(64, n_actions),\n",
    "            #nn.Softmax(dim=1)\n",
    "        #)\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 126)\n",
    "        self.fc3 = nn.Linear(126, 64)\n",
    "        self.fc4 = nn.Linear(64, n_actions)\n",
    "    # forward propagation function\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = PolicyNetwork(3, 3)\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "model = PolicyNetwork(3, 3)\n",
    "model.fc2.register_forward_hook(get_activation('fc2'))\n",
    "x = torch.randn(1, 3)\n",
    "output = model(x)\n",
    "\n",
    "def get_feature(net, x):\n",
    "    activation = {}\n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            activation[name] = output.detach()\n",
    "        return hook\n",
    "    net.fc1.register_forward_hook(get_activation('fc1'))\n",
    "    net.fc2.register_forward_hook(get_activation('fc2'))\n",
    "    net.fc3.register_forward_hook(get_activation('fc3'))\n",
    "    net.fc4.register_forward_hook(get_activation('fc4'))\n",
    "    output = net(x)\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = PolicyNetwork(3, 3)\n",
    "x = torch.randn(1, 3)\n",
    "out = get_feature(net, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_q_vals(rewards):\n",
    "    \"\"\"\n",
    "    rewards: list of rewards collected in episode\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    sum_r = 0.0\n",
    "    # looping through the list of rewards in reverse order\n",
    "    # because in REINFORCE (look at report pseudocode)\n",
    "    # last action receives one last reward, pre-last action\n",
    "    # receives pre-last reward plus discounted last etc.:\n",
    "    for r in reversed(rewards):\n",
    "        sum_r *= 0.99\n",
    "        sum_r += r\n",
    "        res.append(sum_r)\n",
    "    # we againg reverse our list of rewards per action\n",
    "    # in the order actions appeared\n",
    "    return list(reversed(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = np.finfo(float).eps\n",
    "#array_loads = df['load'].diff(24).diff(24*7).diff(24*7*4).diff(24*7*4*12)\n",
    "array_loads = df['load']\n",
    "array_weekdays = df['weekday']+1\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "new_data = scaler.fit_transform(array_loads.values[:, np.newaxis])\n",
    "new_data = new_data.reshape(1,-1)[0]\n",
    "array_loads = new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112951    1\n",
      "112952    1\n",
      "112953    1\n",
      "112954    1\n",
      "112955    1\n",
      "112956    1\n",
      "112957    1\n",
      "112958    1\n",
      "112959    1\n",
      "112960    1\n",
      "112961    1\n",
      "112962    1\n",
      "112963    1\n",
      "112964    1\n",
      "112965    1\n",
      "112966    1\n",
      "112967    1\n",
      "112968    2\n",
      "112969    2\n",
      "112970    2\n",
      "112971    2\n",
      "112972    2\n",
      "112973    2\n",
      "112974    2\n",
      "112975    2\n",
      "112976    2\n",
      "112977    2\n",
      "112978    2\n",
      "112979    2\n",
      "112980    2\n",
      "112981    2\n",
      "112982    2\n",
      "112983    2\n",
      "112984    2\n",
      "112985    2\n",
      "112986    2\n",
      "112987    2\n",
      "112988    2\n",
      "112989    2\n",
      "112990    2\n",
      "112991    2\n",
      "112992    3\n",
      "112993    3\n",
      "112994    3\n",
      "112995    3\n",
      "112996    3\n",
      "112997    3\n",
      "112998    3\n",
      "112999    3\n",
      "113000    3\n",
      "113001    3\n",
      "113002    3\n",
      "113003    3\n",
      "113004    3\n",
      "113005    3\n",
      "113006    3\n",
      "113007    3\n",
      "113008    3\n",
      "113009    3\n",
      "113010    3\n",
      "113011    3\n",
      "113012    3\n",
      "113013    3\n",
      "113014    3\n",
      "113015    3\n",
      "113016    4\n",
      "113017    4\n",
      "113018    4\n",
      "113019    4\n",
      "113020    4\n",
      "113021    4\n",
      "113022    4\n",
      "113023    4\n",
      "113024    4\n",
      "113025    4\n",
      "113026    4\n",
      "113027    4\n",
      "113028    4\n",
      "113029    4\n",
      "113030    4\n",
      "113031    4\n",
      "113032    4\n",
      "113033    4\n",
      "113034    4\n",
      "113035    4\n",
      "113036    4\n",
      "113037    4\n",
      "113038    4\n",
      "113039    4\n",
      "113040    5\n",
      "113041    5\n",
      "113042    5\n",
      "113043    5\n",
      "113044    5\n",
      "113045    5\n",
      "113046    5\n",
      "113047    5\n",
      "113048    5\n",
      "113049    5\n",
      "113050    5\n",
      "113051    5\n",
      "113052    5\n",
      "113053    5\n",
      "113054    5\n",
      "113055    5\n",
      "113056    5\n",
      "113057    5\n",
      "113058    5\n",
      "113059    5\n",
      "113060    5\n",
      "113061    5\n",
      "113062    5\n",
      "113063    5\n",
      "113064    6\n",
      "113065    6\n",
      "113066    6\n",
      "113067    6\n",
      "113068    6\n",
      "113069    6\n",
      "113070    6\n",
      "113071    6\n",
      "113072    6\n",
      "113073    6\n",
      "113074    6\n",
      "113075    6\n",
      "113076    6\n",
      "113077    6\n",
      "113078    6\n",
      "113079    6\n",
      "113080    6\n",
      "113081    6\n",
      "113082    6\n",
      "113083    6\n",
      "113084    6\n",
      "113085    6\n",
      "113086    6\n",
      "113087    6\n",
      "113088    7\n",
      "113089    7\n",
      "113090    7\n",
      "113091    7\n",
      "113092    7\n",
      "113093    7\n",
      "113094    7\n",
      "113095    7\n",
      "113096    7\n",
      "113097    7\n",
      "113098    7\n",
      "113099    7\n",
      "113100    7\n",
      "113101    7\n",
      "113102    7\n",
      "113103    7\n",
      "113104    7\n",
      "113105    7\n",
      "113106    7\n",
      "113107    7\n",
      "113108    7\n",
      "113109    7\n",
      "113110    7\n",
      "113111    7\n",
      "113112    1\n",
      "113113    1\n",
      "113114    1\n",
      "113115    1\n",
      "113116    1\n",
      "113117    1\n",
      "113118    1\n",
      "113119    1\n",
      "113120    1\n",
      "113121    1\n",
      "113122    1\n",
      "113123    1\n",
      "113124    1\n",
      "113125    1\n",
      "113126    1\n",
      "113127    1\n",
      "113128    1\n",
      "113129    1\n",
      "113130    1\n",
      "113131    1\n",
      "113132    1\n",
      "113133    1\n",
      "113134    1\n",
      "113135    1\n",
      "113136    2\n",
      "113137    2\n",
      "113138    2\n",
      "113139    2\n",
      "113140    2\n",
      "113141    2\n",
      "113142    2\n",
      "113143    2\n",
      "113144    2\n",
      "113145    2\n",
      "113146    2\n",
      "113147    2\n",
      "113148    2\n",
      "113149    2\n",
      "113150    2\n",
      "113151    2\n",
      "113152    2\n",
      "113153    2\n",
      "113154    2\n",
      "113155    2\n",
      "113156    2\n",
      "113157    2\n",
      "113158    2\n",
      "113159    2\n",
      "113160    3\n",
      "113161    3\n",
      "113162    3\n",
      "113163    3\n",
      "113164    3\n",
      "113165    3\n",
      "113166    3\n",
      "113167    3\n",
      "113168    3\n",
      "113169    3\n",
      "113170    3\n",
      "113171    3\n",
      "113172    3\n",
      "113173    3\n",
      "113174    3\n",
      "113175    3\n",
      "113176    3\n",
      "113177    3\n",
      "113178    3\n",
      "113179    3\n",
      "113180    3\n",
      "113181    3\n",
      "113182    3\n",
      "113183    3\n",
      "113184    4\n",
      "113185    4\n",
      "113186    4\n",
      "113187    4\n",
      "113188    4\n",
      "113189    4\n",
      "113190    4\n",
      "113191    4\n",
      "113192    4\n",
      "113193    4\n",
      "113194    4\n",
      "113195    4\n",
      "113196    4\n",
      "113197    4\n",
      "113198    4\n",
      "113199    4\n",
      "113200    4\n",
      "113201    4\n",
      "113202    4\n",
      "113203    4\n",
      "113204    4\n",
      "113205    4\n",
      "113206    4\n",
      "113207    4\n",
      "113208    5\n",
      "113209    5\n",
      "113210    5\n",
      "113211    5\n",
      "113212    5\n",
      "113213    5\n",
      "113214    5\n",
      "113215    5\n",
      "113216    5\n",
      "113217    5\n",
      "113218    5\n",
      "113219    5\n",
      "113220    5\n",
      "113221    5\n",
      "113222    5\n",
      "113223    5\n",
      "113224    5\n",
      "113225    5\n",
      "113226    5\n",
      "113227    5\n",
      "113228    5\n",
      "113229    5\n",
      "113230    5\n",
      "113231    5\n",
      "113232    6\n",
      "113233    6\n",
      "113234    6\n",
      "113235    6\n",
      "113236    6\n",
      "113237    6\n",
      "113238    6\n",
      "113239    6\n",
      "113240    6\n",
      "113241    6\n",
      "113242    6\n",
      "113243    6\n",
      "113244    6\n",
      "113245    6\n",
      "113246    6\n",
      "113247    6\n",
      "113248    6\n",
      "113249    6\n",
      "113250    6\n",
      "113251    6\n",
      "113252    6\n",
      "113253    6\n",
      "113254    6\n",
      "113255    6\n",
      "113256    7\n",
      "113257    7\n",
      "113258    7\n",
      "113259    7\n",
      "113260    7\n",
      "113261    7\n",
      "113262    7\n",
      "113263    7\n",
      "113264    7\n",
      "113265    7\n",
      "113266    7\n",
      "113267    7\n",
      "113268    7\n",
      "113269    7\n",
      "113270    7\n",
      "113271    7\n",
      "113272    7\n",
      "113273    7\n",
      "113274    7\n",
      "113275    7\n",
      "113276    7\n",
      "113277    7\n",
      "113278    7\n",
      "113279    7\n",
      "113280    1\n",
      "113281    1\n",
      "113282    1\n",
      "113283    1\n",
      "113284    1\n",
      "113285    1\n",
      "113286    1\n",
      "113287    1\n",
      "113288    1\n",
      "113289    1\n",
      "113290    1\n",
      "113291    1\n",
      "113292    1\n",
      "113293    1\n",
      "113294    1\n",
      "113295    1\n",
      "113296    1\n",
      "113297    1\n",
      "113298    1\n",
      "113299    1\n",
      "113300    1\n",
      "113301    1\n",
      "113302    1\n",
      "113303    1\n",
      "113304    2\n",
      "113305    2\n",
      "113306    2\n",
      "113307    2\n",
      "113308    2\n",
      "113309    2\n",
      "113310    2\n",
      "113311    2\n",
      "113312    2\n",
      "113313    2\n",
      "113314    2\n",
      "113315    2\n",
      "113316    2\n",
      "113317    2\n",
      "113318    2\n",
      "113319    2\n",
      "113320    2\n",
      "113321    2\n",
      "113322    2\n",
      "113323    2\n",
      "113324    2\n",
      "113325    2\n",
      "113326    2\n",
      "113327    2\n",
      "113328    3\n",
      "113329    3\n",
      "113330    3\n",
      "113331    3\n",
      "113332    3\n",
      "113333    3\n",
      "113334    3\n",
      "113335    3\n",
      "113336    3\n",
      "113337    3\n",
      "113338    3\n",
      "113339    3\n",
      "113340    3\n",
      "113341    3\n",
      "113342    3\n",
      "113343    3\n",
      "113344    3\n",
      "113345    3\n",
      "113346    3\n",
      "113347    3\n",
      "113348    3\n",
      "113349    3\n",
      "113350    3\n",
      "113351    3\n",
      "113352    4\n",
      "113353    4\n",
      "113354    4\n",
      "113355    4\n",
      "113356    4\n",
      "113357    4\n",
      "113358    4\n",
      "113359    4\n",
      "113360    4\n",
      "113361    4\n",
      "113362    4\n",
      "113363    4\n",
      "113364    4\n",
      "113365    4\n",
      "113366    4\n",
      "113367    4\n",
      "113368    4\n",
      "113369    4\n",
      "113370    4\n",
      "113371    4\n",
      "113372    4\n",
      "113373    4\n",
      "113374    4\n",
      "113375    4\n",
      "113376    5\n",
      "113377    5\n",
      "113378    5\n",
      "113379    5\n",
      "113380    5\n",
      "113381    5\n",
      "113382    5\n",
      "113383    5\n",
      "113384    5\n",
      "113385    5\n",
      "113386    5\n",
      "113387    5\n",
      "113388    5\n",
      "113389    5\n",
      "113390    5\n",
      "113391    5\n",
      "113392    5\n",
      "113393    5\n",
      "113394    5\n",
      "113395    5\n",
      "113396    5\n",
      "113397    5\n",
      "113398    5\n",
      "113399    5\n",
      "113400    6\n",
      "113401    6\n",
      "113402    6\n",
      "113403    6\n",
      "113404    6\n",
      "113405    6\n",
      "113406    6\n",
      "113407    6\n",
      "113408    6\n",
      "113409    6\n",
      "113410    6\n",
      "113411    6\n",
      "113412    6\n",
      "113413    6\n",
      "113414    6\n",
      "113415    6\n",
      "113416    6\n",
      "113417    6\n",
      "113418    6\n",
      "113419    6\n",
      "113420    6\n",
      "113421    6\n",
      "113422    6\n",
      "113423    6\n",
      "113424    7\n",
      "113425    7\n",
      "113426    7\n",
      "113427    7\n",
      "113428    7\n",
      "113429    7\n",
      "113430    7\n",
      "113431    7\n",
      "113432    7\n",
      "113433    7\n",
      "113434    7\n",
      "113435    7\n",
      "113436    7\n",
      "113437    7\n",
      "113438    7\n",
      "113439    7\n",
      "113440    7\n",
      "113441    7\n",
      "113442    7\n",
      "113443    7\n",
      "113444    7\n",
      "113445    7\n",
      "113446    7\n",
      "113447    7\n",
      "113448    1\n",
      "113449    1\n",
      "113450    1\n",
      "113451    1\n",
      "113452    1\n",
      "113453    1\n",
      "113454    1\n",
      "113455    1\n",
      "113456    1\n",
      "113457    1\n",
      "113458    1\n",
      "113459    1\n",
      "113460    1\n",
      "113461    1\n",
      "113462    1\n",
      "113463    1\n",
      "113464    1\n",
      "113465    1\n",
      "113466    1\n",
      "113467    1\n",
      "113468    1\n",
      "113469    1\n",
      "113470    1\n",
      "113471    1\n",
      "113472    2\n",
      "113473    2\n",
      "113474    2\n",
      "113475    2\n",
      "113476    2\n",
      "113477    2\n",
      "113478    2\n",
      "113479    2\n",
      "113480    2\n",
      "113481    2\n",
      "113482    2\n",
      "113483    2\n",
      "113484    2\n",
      "113485    2\n",
      "113486    2\n",
      "113487    2\n",
      "113488    2\n",
      "113489    2\n",
      "113490    2\n",
      "113491    2\n",
      "113492    2\n",
      "113493    2\n",
      "113494    2\n",
      "113495    2\n",
      "113496    3\n",
      "113497    3\n",
      "113498    3\n",
      "113499    3\n",
      "113500    3\n",
      "113501    3\n",
      "113502    3\n",
      "113503    3\n",
      "113504    3\n",
      "113505    3\n",
      "113506    3\n",
      "113507    3\n",
      "113508    3\n",
      "113509    3\n",
      "113510    3\n",
      "113511    3\n",
      "113512    3\n",
      "113513    3\n",
      "113514    3\n",
      "113515    3\n",
      "113516    3\n",
      "113517    3\n",
      "113518    3\n",
      "113519    3\n",
      "113520    4\n",
      "113521    4\n",
      "113522    4\n",
      "113523    4\n",
      "113524    4\n",
      "113525    4\n",
      "113526    4\n",
      "113527    4\n",
      "113528    4\n",
      "113529    4\n",
      "113530    4\n",
      "113531    4\n",
      "113532    4\n",
      "113533    4\n",
      "113534    4\n",
      "113535    4\n",
      "113536    4\n",
      "113537    4\n",
      "113538    4\n",
      "113539    4\n",
      "113540    4\n",
      "113541    4\n",
      "113542    4\n",
      "113543    4\n",
      "113544    5\n",
      "113545    5\n",
      "113546    5\n",
      "113547    5\n",
      "113548    5\n",
      "113549    5\n",
      "113550    5\n",
      "113551    5\n",
      "113552    5\n",
      "113553    5\n",
      "113554    5\n",
      "113555    5\n",
      "113556    5\n",
      "113557    5\n",
      "113558    5\n",
      "113559    5\n",
      "113560    5\n",
      "113561    5\n",
      "113562    5\n",
      "113563    5\n",
      "113564    5\n",
      "113565    5\n",
      "113566    5\n",
      "113567    5\n",
      "113568    6\n",
      "113569    6\n",
      "113570    6\n",
      "113571    6\n",
      "113572    6\n",
      "113573    6\n",
      "113574    6\n",
      "113575    6\n",
      "113576    6\n",
      "113577    6\n",
      "113578    6\n",
      "113579    6\n",
      "113580    6\n",
      "113581    6\n",
      "113582    6\n",
      "113583    6\n",
      "113584    6\n",
      "113585    6\n",
      "113586    6\n",
      "113587    6\n",
      "113588    6\n",
      "113589    6\n",
      "113590    6\n",
      "113591    6\n",
      "113592    7\n",
      "113593    7\n",
      "113594    7\n",
      "113595    7\n",
      "113596    7\n",
      "113597    7\n",
      "113598    7\n",
      "113599    7\n",
      "113600    7\n",
      "113601    7\n",
      "113602    7\n",
      "113603    7\n",
      "113604    7\n",
      "113605    7\n",
      "113606    7\n",
      "113607    7\n",
      "113608    7\n",
      "113609    7\n",
      "113610    7\n",
      "113611    7\n",
      "113612    7\n",
      "113613    7\n",
      "113614    7\n",
      "113615    7\n",
      "113616    1\n",
      "113617    1\n",
      "113618    1\n",
      "113619    1\n",
      "113620    1\n",
      "113621    1\n",
      "113622    1\n",
      "113623    1\n",
      "113624    1\n",
      "113625    1\n",
      "113626    1\n",
      "113627    1\n",
      "113628    1\n",
      "113629    1\n",
      "113630    1\n",
      "113631    1\n",
      "113632    1\n",
      "113633    1\n",
      "113634    1\n",
      "113635    1\n",
      "113636    1\n",
      "113637    1\n",
      "113638    1\n",
      "113639    1\n",
      "113640    2\n",
      "113641    2\n",
      "113642    2\n",
      "113643    2\n",
      "113644    2\n",
      "113645    2\n",
      "113646    2\n",
      "113647    2\n",
      "113648    2\n",
      "113649    2\n",
      "113650    2\n",
      "113651    2\n",
      "113652    2\n",
      "113653    2\n",
      "113654    2\n",
      "113655    2\n",
      "113656    2\n",
      "113657    2\n",
      "113658    2\n",
      "113659    2\n",
      "113660    2\n",
      "113661    2\n",
      "113662    2\n",
      "113663    2\n",
      "113664    3\n",
      "113665    3\n",
      "113666    3\n",
      "113667    3\n",
      "113668    3\n",
      "113669    3\n",
      "113670    3\n",
      "113671    3\n",
      "113672    3\n",
      "113673    3\n",
      "113674    3\n",
      "113675    3\n",
      "113676    3\n",
      "113677    3\n",
      "113678    3\n",
      "113679    3\n",
      "113680    3\n",
      "113681    3\n",
      "113682    3\n",
      "113683    3\n",
      "113684    3\n",
      "113685    3\n",
      "113686    3\n",
      "113687    3\n",
      "113688    4\n",
      "113689    4\n",
      "113690    4\n",
      "113691    4\n",
      "113692    4\n",
      "113693    4\n",
      "113694    4\n",
      "113695    4\n",
      "113696    4\n",
      "113697    4\n",
      "113698    4\n",
      "113699    4\n",
      "113700    4\n",
      "113701    4\n",
      "113702    4\n",
      "113703    4\n",
      "113704    4\n",
      "113705    4\n",
      "113706    4\n",
      "113707    4\n",
      "113708    4\n",
      "113709    4\n",
      "113710    4\n",
      "113711    4\n",
      "113712    5\n",
      "113713    5\n",
      "113714    5\n",
      "113715    5\n",
      "113716    5\n",
      "113717    5\n",
      "113718    5\n",
      "113719    5\n",
      "113720    5\n",
      "113721    5\n",
      "113722    5\n",
      "113723    5\n",
      "113724    5\n",
      "113725    5\n",
      "113726    5\n",
      "113727    5\n",
      "113728    5\n",
      "113729    5\n",
      "113730    5\n",
      "113731    5\n",
      "113732    5\n",
      "113733    5\n",
      "113734    5\n",
      "113735    5\n",
      "113736    6\n",
      "113737    6\n",
      "113738    6\n",
      "113739    6\n",
      "113740    6\n",
      "113741    6\n",
      "113742    6\n",
      "113743    6\n",
      "113744    6\n",
      "113745    6\n",
      "113746    6\n",
      "113747    6\n",
      "113748    6\n",
      "113749    6\n",
      "113750    6\n",
      "113751    6\n",
      "113752    6\n",
      "113753    6\n",
      "113754    6\n",
      "113755    6\n",
      "113756    6\n",
      "113757    6\n",
      "113758    6\n",
      "113759    6\n",
      "113760    7\n",
      "113761    7\n",
      "113762    7\n",
      "113763    7\n",
      "113764    7\n",
      "113765    7\n",
      "113766    7\n",
      "113767    7\n",
      "113768    7\n",
      "113769    7\n",
      "113770    7\n",
      "113771    7\n",
      "113772    7\n",
      "113773    7\n",
      "113774    7\n",
      "113775    7\n",
      "113776    7\n",
      "113777    7\n",
      "113778    7\n",
      "113779    7\n",
      "113780    7\n",
      "113781    7\n",
      "113782    7\n",
      "113783    7\n",
      "113784    1\n",
      "113785    1\n",
      "113786    1\n",
      "113787    1\n",
      "113788    1\n",
      "113789    1\n",
      "113790    1\n",
      "113791    1\n",
      "113792    1\n",
      "113793    1\n",
      "113794    1\n",
      "113795    1\n",
      "113796    1\n",
      "113797    1\n",
      "113798    1\n",
      "113799    1\n",
      "113800    1\n",
      "113801    1\n",
      "113802    1\n",
      "113803    1\n",
      "113804    1\n",
      "113805    1\n",
      "113806    1\n",
      "113807    1\n",
      "113808    2\n",
      "113809    2\n",
      "113810    2\n",
      "113811    2\n",
      "113812    2\n",
      "113813    2\n",
      "113814    2\n",
      "113815    2\n",
      "113816    2\n",
      "113817    2\n",
      "113818    2\n",
      "113819    2\n",
      "113820    2\n",
      "113821    2\n",
      "113822    2\n",
      "113823    2\n",
      "113824    2\n",
      "113825    2\n",
      "113826    2\n",
      "113827    2\n",
      "113828    2\n",
      "113829    2\n",
      "113830    2\n",
      "113831    2\n",
      "113832    3\n",
      "113833    3\n",
      "113834    3\n",
      "113835    3\n",
      "113836    3\n",
      "113837    3\n",
      "113838    3\n",
      "113839    3\n",
      "113840    3\n",
      "113841    3\n",
      "113842    3\n",
      "113843    3\n",
      "113844    3\n",
      "113845    3\n",
      "113846    3\n",
      "113847    3\n",
      "113848    3\n",
      "113849    3\n",
      "113850    3\n",
      "113851    3\n",
      "113852    3\n",
      "113853    3\n",
      "113854    3\n",
      "113855    3\n",
      "113856    4\n",
      "113857    4\n",
      "113858    4\n",
      "113859    4\n",
      "113860    4\n",
      "113861    4\n",
      "113862    4\n",
      "113863    4\n",
      "113864    4\n",
      "113865    4\n",
      "113866    4\n",
      "113867    4\n",
      "113868    4\n",
      "113869    4\n",
      "113870    4\n",
      "113871    4\n",
      "113872    4\n",
      "113873    4\n",
      "113874    4\n",
      "113875    4\n",
      "113876    4\n",
      "113877    4\n",
      "113878    4\n",
      "113879    4\n",
      "113880    5\n",
      "113881    5\n",
      "113882    5\n",
      "113883    5\n",
      "113884    5\n",
      "113885    5\n",
      "113886    5\n",
      "113887    5\n",
      "113888    5\n",
      "113889    5\n",
      "113890    5\n",
      "113891    5\n",
      "113892    5\n",
      "113893    5\n",
      "113894    5\n",
      "113895    5\n",
      "113896    5\n",
      "113897    5\n",
      "113898    5\n",
      "113899    5\n",
      "113900    5\n",
      "113901    5\n",
      "113902    5\n",
      "113903    5\n",
      "113904    6\n",
      "113905    6\n",
      "113906    6\n",
      "113907    6\n",
      "113908    6\n",
      "113909    6\n",
      "113910    6\n",
      "113911    6\n",
      "113912    6\n",
      "113913    6\n",
      "113914    6\n",
      "113915    6\n",
      "113916    6\n",
      "113917    6\n",
      "113918    6\n",
      "113919    6\n",
      "113920    6\n",
      "113921    6\n",
      "113922    6\n",
      "113923    6\n",
      "113924    6\n",
      "113925    6\n",
      "113926    6\n",
      "113927    6\n",
      "113928    7\n",
      "113929    7\n",
      "113930    7\n",
      "113931    7\n",
      "113932    7\n",
      "113933    7\n",
      "113934    7\n",
      "113935    7\n",
      "113936    7\n",
      "113937    7\n",
      "113938    7\n",
      "113939    7\n",
      "113940    7\n",
      "113941    7\n",
      "113942    7\n",
      "113943    7\n",
      "113944    7\n",
      "113945    7\n",
      "113946    7\n",
      "113947    7\n",
      "113948    7\n",
      "113949    7\n",
      "113950    7\n",
      "Name: weekday, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "array_loads_test = array_loads[-1000:]\n",
    "array_weekdays_test = array_weekdays[-1000:]\n",
    "print(array_weekdays_test)\n",
    "array_loads_train = array_loads[:-11395]\n",
    "array_weekdays_train = array_weekdays[:-11395]\n",
    "array_loads_train_conv = array_loads[:-100000]\n",
    "array_weekdays_train_conv = array_weekdays[:-100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(lr, n, m, clip_grad=False):\n",
    "    if clip_grad:\n",
    "        CLIP_GRAD=0.1\n",
    "    #STARTING EPISODE\n",
    "    env1 = Env(array_loads_train, array_weekdays_train)\n",
    "    env1.generate_input_load()\n",
    "    env1.generate_input_pv()\n",
    "    env1.calculate_actual_load()\n",
    "\n",
    "    #number of episodes (days) used for 1 backprop\n",
    "    n=n\n",
    "    loss_list=[]\n",
    "    action_list=[]\n",
    "    norm_grad_list = []\n",
    "    mean_rewards_list = []\n",
    "    torch.manual_seed(0)\n",
    "    net = PolicyNetwork(3, 3)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    #create list for collecting total rewards\n",
    "    total_rewards = []\n",
    "\n",
    "    map_action = {0:'discharge',1:'charge',2:'wait'}\n",
    "\n",
    "    batch_states, batch_actions, batch_qvals, batch_times = [], [], [], []\n",
    "    cur_rewards = []\n",
    "    np.random.seed(0)\n",
    "    for j in range(0, m):\n",
    "        env1.clear()\n",
    "        env1.generate_input_load()\n",
    "        env1.generate_input_pv()\n",
    "        env1.calculate_actual_load()\n",
    "        #years of data for pass\n",
    "        for i in range(len(array_loads_train) - 1):\n",
    "            obs = np.array([env1.actual_load])\n",
    "            obs_v = torch.FloatTensor(obs)\n",
    "            batch_states.append([env1.load_idx % 24, obs, env1.input_pv])\n",
    "            logits_v = net(torch.tensor([env1.load_idx % 24,obs_v, env1.input_pv]))\n",
    "            probs = F.softmax(logits_v).detach().numpy()\n",
    "            action = np.random.choice(np.array([0,1,2]),p=np.array(probs).round(4))\n",
    "            env1.action=map_action[action]\n",
    "            action_list.append(env1.action)\n",
    "            env1.generate_input_load()\n",
    "            env1.generate_input_pv()\n",
    "            env1.calculate_actual_load()\n",
    "            env1.calculate_reward()\n",
    "            batch_actions.append(np.array([action]))\n",
    "            cur_rewards.append(env1.reward)\n",
    "            if env1.episode<n:\n",
    "                continue\n",
    "\n",
    "            cur_rewards = np.array(cur_rewards)\n",
    "            #cur_rewards = (cur_rewards - cur_rewards.mean()) / (cur_rewards.std()+eps)\n",
    "            total_rewards.append(sum(cur_rewards)/n)\n",
    "            mean_reward = float(np.mean(total_rewards[-7:]))\n",
    "            mean_rewards_list.append(mean_reward)\n",
    "            optimizer.zero_grad()\n",
    "            batch_qvals.extend(compute_q_vals(cur_rewards))\n",
    "            cur_rewards=list(cur_rewards)\n",
    "            cur_rewards.clear()\n",
    "\n",
    "            states_v = torch.FloatTensor(batch_states)\n",
    "            batch_actions_t = torch.LongTensor(batch_actions)\n",
    "            batch_qvals_v = torch.FloatTensor(batch_qvals)\n",
    "\n",
    "            # neural network, log softmax, q_vals * log_softmax, loss, backprop\n",
    "            logits_v = net(states_v)\n",
    "            log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "            out, _ = torch.max(batch_actions_t,dim=1)\n",
    "            log_prob_actions_v = batch_qvals_v*log_prob_v[range(len(batch_states)),out]\n",
    "            loss_v = -log_prob_actions_v.mean()\n",
    "            #loss_list.append(loss_v)  \n",
    "            loss_v.backward()\n",
    "            #nn.utils.clip_grad_norm_(net.parameters(), CLIP_GRAD)\n",
    "            optimizer.step()\n",
    "\n",
    "            #let's find grads and grads norm and variance\n",
    "            grads = np.concatenate([p.grad.data.cpu().numpy().flatten()\n",
    "                                                            for p in net.parameters()\n",
    "                                                            if p.grad is not None])\n",
    "            norm_grad = np.linalg.norm(grads)\n",
    "            norm_grad_list.append(norm_grad)\n",
    "            batch_states.clear()\n",
    "            batch_actions.clear()\n",
    "            batch_qvals.clear()\n",
    "            env1.episode = 0\n",
    "    return env1,mean_rewards_list,total_rewards, net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, mean_rewards_list, total_rewards, policy=main(0.0003, 30, 5)\n",
    "#torch.save(net.state_dict(), 'strange_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(mean_rewards_list)),mean_rewards_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "def profile_DNN(net, array_loads, array_weekdays, array_weekdays_idx):\n",
    "    \"\"\"\n",
    "    Profile the DNN and return a cov_dict contains the necessary information\n",
    "    ---\n",
    "    Args\n",
    "        net: torch.nn.Module Deep learning model.\n",
    "        dataloader: torch.utils.data.DataLoader The pytorch dataloader.\n",
    "    Returns:\n",
    "        cov_dict: a dict contains (layer_name, neuron_id) --> (neurons_min, neurons_max) mapping.\n",
    "    \n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    cov_dict = collections.OrderedDict()\n",
    "    env1 = Env(array_loads, array_weekdays)\n",
    "    env1.day_idx = array_weekdays_idx\n",
    "    map_action = {0:'discharge',1:'charge',2:'wait'}\n",
    "    name_list = ['fc1', 'fc2', 'fc3', 'fc4']\n",
    "    batch_num = len(array_loads)\n",
    "    for i in range(len(array_loads) - 1):\n",
    "        env1.generate_input_load()\n",
    "        env1.generate_input_pv()\n",
    "        env1.calculate_actual_load()\n",
    "        env1.calculate_reward()\n",
    "        obs = np.array([env1.actual_load])\n",
    "        obs_v = torch.FloatTensor(obs)\n",
    "        logits_v = net(torch.tensor([env1.load_idx % 24,obs_v, env1.input_pv]))\n",
    "        probs = F.softmax(logits_v).detach().numpy()\n",
    "        action = np.random.choice(np.array([0,1,2]),p=np.array(probs).round(4))\n",
    "        env1.action=map_action[action]\n",
    "        with torch.no_grad():\n",
    "            out_list = get_feature(net, torch.tensor([env1.load_idx % 24,obs_v, env1.input_pv]))\n",
    "        if i % 10000 == 0:\n",
    "            print(\"* finish {}/{} proportion\".format(i, batch_num))\n",
    "        # Iterate through all output layers in the output list\n",
    "        for layer_id in range(len(out_list)):\n",
    "            cur_layer = name_list[layer_id] # record the name of current layer\n",
    "            \n",
    "            '''\n",
    "            Recall the definition of neurons. For a linear layer, the output is [Batch_size, neurons]\n",
    "            While for a convolutional layer, the output is [Batch_size, channel, H, w]\n",
    "            As we define that every entry in the 3D output volume is one neuron, \n",
    "            the number of neurons in conv layer is just channel * H * W.\n",
    "            '''\n",
    "            \n",
    "            cur_neuron_num = 1\n",
    "            \n",
    "            '''\n",
    "            Please compute the number of neurons in the DNN and store the result in cur_neuron_num.\n",
    "            '''\n",
    "            # ======Below is TODO====== #\n",
    "            if len(list(out_list[cur_layer].shape)) == 4:\n",
    "              N, C, H, W = out_list[cur_layer].shape\n",
    "              cur_neuron_num = C * H * W\n",
    "            elif len(list(out_list[cur_layer].shape)) == 2:\n",
    "              N, cur_neuron_num = out_list[cur_layer].shape\n",
    "            elif len(list(out_list[cur_layer].shape)) == 1:\n",
    "              N, cur_neuron_num = 1, out_list[cur_layer].shape[0]\n",
    "            # ======Above is TODO====== #\n",
    "            \n",
    "            '''\n",
    "            Please records the max value and min value over **batch** (the first dimension) from the **out_list**\n",
    "            Hint: you can use torch.flatten or reshape to turn a multi-dimension tensor\n",
    "            into one-dimension tensor to [C, H, W] --> [N] can be easily used to represent neurons.\n",
    "            Data structure used here: out_list, neurons_max, neurons_min\n",
    "            '''\n",
    "            \n",
    "            # ======Below is TODO====== #\n",
    "            flat = torch.reshape(out_list[cur_layer], (N, cur_neuron_num))\n",
    "            neurons_max, _ = torch.max(flat, dim=0)\n",
    "            neurons_min, _ = torch.min(flat, dim=0)\n",
    "            # ======Above is TODO====== #\n",
    "\n",
    "            for neuron_id in range(cur_neuron_num):\n",
    "                # Please record the range of the neuron for key (cur_layer, neuron_id).\n",
    "                if (cur_layer, neuron_id) not in cov_dict:\n",
    "                    # The fisrt time to record the pair.\n",
    "                    cov_dict[(cur_layer, neuron_id)] = [None, None] # Initialize [Lower, upper]\n",
    "                \n",
    "                profile_data_list = cov_dict[(cur_layer, neuron_id)]\n",
    "\n",
    "                '''\n",
    "                Please compare the result in neurons_max, neurons_min with that in profile_data_list\n",
    "                If neurons_max is larger than the data in profile_data_list\n",
    "                store it in the upper_bound and store back to the profile_data_list.\n",
    "                Data structure used here: lower_bound, upper_bound, neurons_min, neurons_max\n",
    "                '''\n",
    "                \n",
    "                # ======Below is TODO====== #\n",
    "                lower_bound, upper_bound = profile_data_list\n",
    "                if lower_bound == None:\n",
    "                  lower_bound = neurons_min[neuron_id]\n",
    "                else:\n",
    "                  lower_bound = min(lower_bound, neurons_min[neuron_id])\n",
    "\n",
    "                if upper_bound == None:\n",
    "                  upper_bound = neurons_max[neuron_id]\n",
    "                else:\n",
    "                  upper_bound = max(upper_bound, neurons_max[neuron_id])\n",
    "                \n",
    "                # ======Above is TODO====== #\n",
    "                \n",
    "                profile_data_list[0] = lower_bound\n",
    "                profile_data_list[1] = upper_bound\n",
    "                cov_dict[(cur_layer, neuron_id)] = profile_data_list\n",
    "            \n",
    "    return cov_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* finish 0/13951 proportion\n",
      "* finish 10000/13951 proportion\n"
     ]
    }
   ],
   "source": [
    "cov_dict = profile_DNN(policy, array_loads_train_conv, array_weekdays_train_conv, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_NBCov(net, array_loads, array_weekdays, cov_dict, array_weekdays_idx):\n",
    "    \"\"\"\n",
    "    Compute the Neuron Boundary Coverage metric on a given data loader. \n",
    "    ---\n",
    "    Args\n",
    "        net: torch.nn.Module Deep learning model.\n",
    "        dataloader: torch.utils.data.DataLoader The pytorch dataloader.\n",
    "        cov_dict: a dict contains (layer_name, neuron_id) --> (neurons_min, neurons_max) mapping.\n",
    "    Returns:\n",
    "        result: the coverage result\n",
    "    \n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    coverage_recorder = collections.OrderedDict()\n",
    "    env1 = Env(array_loads, array_weekdays)\n",
    "    env1.day_idx = array_weekdays_idx\n",
    "    map_action = {0:'discharge',1:'charge',2:'wait'}\n",
    "    name_list = ['fc1', 'fc2', 'fc3', 'fc4']\n",
    "    batch_num = len(array_loads)\n",
    "    \n",
    "    for i in range(len(array_loads) - 2):\n",
    "        env1.generate_input_load()\n",
    "        env1.generate_input_pv()\n",
    "        env1.calculate_actual_load()\n",
    "        env1.calculate_reward()\n",
    "        obs = np.array([env1.actual_load])\n",
    "        obs_v = torch.FloatTensor(obs)\n",
    "        logits_v = net(torch.tensor([env1.load_idx % 24,obs_v, env1.input_pv]))\n",
    "        probs = F.softmax(logits_v).detach().numpy()\n",
    "        action = np.random.choice(np.array([0,1,2]),p=np.array(probs).round(4))\n",
    "        env1.action=map_action[action]\n",
    "        with torch.no_grad():\n",
    "            out_list =  get_feature(net, torch.tensor([env1.load_idx % 24,obs_v, env1.input_pv]))\n",
    "        if i % 100 == 0:\n",
    "            print(\"* finish {}/{} proportion\".format(i, batch_num))\n",
    "        \n",
    "        # Iterate through all output layers in the output list\n",
    "        for layer_id in range(len(out_list)):\n",
    "            # record the name of current layer\n",
    "            cur_layer = name_list[layer_id]\n",
    "            cur_neuron_num = 1\n",
    "            \n",
    "            '''\n",
    "            Please compute the number of neurons in the DNN and store the result in cur_neuron_num.\n",
    "            Compute neurons_max and neurons_min based on **out_list**\n",
    "            '''\n",
    "\n",
    "            # ======Below is TODO====== #\n",
    "            if len(list(out_list[cur_layer].shape)) == 4:\n",
    "              N, C, H, W = out_list[cur_layer].shape\n",
    "              cur_neuron_num = C * H * W\n",
    "            elif len(list(out_list[cur_layer].shape)) == 2:\n",
    "              N, cur_neuron_num = out_list[cur_layer].shape\n",
    "            elif len(list(out_list[cur_layer].shape)) == 1:\n",
    "              N, cur_neuron_num = 1, out_list[cur_layer].shape[0]\n",
    "            flat = torch.reshape(out_list[cur_layer], (N, cur_neuron_num))\n",
    "            neurons_max, _ = torch.max(flat, dim=0)\n",
    "            neurons_min, _ = torch.min(flat, dim=0)\n",
    "            # ======Above is TODO====== #\n",
    "\n",
    "            # Compare the result in cov_dict and record neurons who has been covered.\n",
    "            for neuron_id in range(cur_neuron_num):\n",
    "                if (cur_layer, neuron_id) not in coverage_recorder:\n",
    "                    # Init the coverage_recorder\n",
    "                    # [(cur_layer, neuron_id)] = [0, 0] (LowerCornerNeuron, UpperCornerNeuron)\n",
    "                    coverage_recorder[(cur_layer, neuron_id)] = [0, 0] # LowerCornerNeuron, UpperCornerNeuron\n",
    "                \n",
    "                '''\n",
    "                Compare the output in neurons_min(neurons_max) and and cov_dict\n",
    "                If the neuron is covered in LowerCorner, set LowerCornerNeuron = 1\n",
    "                If the neuron is covered in UpperCorner, set UpperCornerNeuron = 1\n",
    "                Data sturcture used here: cov_dict, neurons_min, neurons_max, coverage_recorder\n",
    "                '''\n",
    "                \n",
    "                # ======Below is TODO====== #\n",
    "                if neurons_max[neuron_id] > cov_dict[(cur_layer, neuron_id)][1]:\n",
    "                  coverage_recorder[(cur_layer, neuron_id)][1] = 1\n",
    "                if neurons_min[neuron_id] < cov_dict[(cur_layer, neuron_id)][0]:\n",
    "                  coverage_recorder[(cur_layer, neuron_id)][0] = 1\n",
    "\n",
    "                # ======Above is TODO====== #\n",
    "    \n",
    "    num_of_coverage_neuron = 0\n",
    "    \n",
    "    '''\n",
    "    Compute the number of neurons that are covered\n",
    "    Data sturcture used here: coverage_recorder, result\n",
    "    '''\n",
    "    \n",
    "    # ======Below is TODO====== #\n",
    "    count = 0\n",
    "    for key, value in coverage_recorder.items():\n",
    "      count += 1\n",
    "      if value[0] == 1:\n",
    "        num_of_coverage_neuron += 1\n",
    "      if value[1] == 1:\n",
    "        num_of_coverage_neuron += 1\n",
    "    result = num_of_coverage_neuron / (count * 2)\n",
    "    # ======Above is TODO====== #\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* finish 0/1000 proportion\n",
      "* finish 100/1000 proportion\n",
      "* finish 200/1000 proportion\n",
      "* finish 300/1000 proportion\n",
      "* finish 400/1000 proportion\n",
      "* finish 500/1000 proportion\n",
      "* finish 600/1000 proportion\n",
      "* finish 700/1000 proportion\n",
      "* finish 800/1000 proportion\n",
      "* finish 900/1000 proportion\n",
      "0.18368794326241134\n"
     ]
    }
   ],
   "source": [
    "realtime_pv = [0, 0, 0, 0, 0, 0, 0.1, 0.2, 0.4, 0.6, 0.7, 0.9, 1, 1, 0.9, 0.8, 0.6, 0.4, 0.1, 0, 0, 0, 0, 0]\n",
    "realtime_pv = [i * 7200 for i in realtime_pv]\n",
    "array_loads_test = array_loads[-1000:]\n",
    "array_weekdays_test = array_weekdays[-1000:]\n",
    "cov_result = compute_NBCov(policy, array_loads_test, array_weekdays_test, cov_dict, 112952)\n",
    "print(cov_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* finish 0/1000 proportion\n",
      "* finish 100/1000 proportion\n",
      "* finish 200/1000 proportion\n",
      "* finish 300/1000 proportion\n"
     ]
    }
   ],
   "source": [
    "realtime_pv = [0, 0, 0, 0, 0, 0, 0.1, 0.2, 0.4, 0.6, 0.7, 0.9, 1, 1, 0.9, 0.8, 0.6, 0.4, 0.1, 0, 0, 0, 0, 0]\n",
    "realtime_pv = [i * 7200 for i in realtime_pv]\n",
    "realtime_pv = [i * 0.8 for i in realtime_pv]  # mutate\n",
    "array_loads_test = array_loads[-1000:]\n",
    "array_loads_test = array_loads_test * 1.2 #mutate\n",
    "cov_result = compute_NBCov(policy, array_loads_test, array_weekdays_test, cov_dict, 112952)\n",
    "print(cov_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
